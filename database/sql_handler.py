from datetime import datetime
import uuid

import pandas as pd
from sqlalchemy import and_

from database.data_handler import DataHandler
from database.database import db_session, Base
from utility.constants import (
    DATA_SOURCE_TYPE,
    DATA_LOCATION,
    LEFT_KEYS,
    RIGHT_KEYS,
    UPLOAD_ID,
    UPLOAD_TIME,
    INDEX_COLUMN,
)


class SqlDataInventory:
    @staticmethod
    def get_available_data_source():
        return list(Base.metadata.tables.keys())

    @staticmethod
    def get_schema_for_data_source(data_source_name):
        """
        :param data_source_name: str
        :return: list of sqlalchemy column objects
        """
        table = Base.metadata.tables[data_source_name]
        schema_columns = table.columns.values()
        return schema_columns

    @staticmethod
    def get_sqlalchemy_model_class_for_data_source_name(data_source_name):
        """

        :param data_source_name:
        :return: sqlalchemy model class
        """
        for c in Base._decl_class_registry.values():
            if hasattr(c, "__tablename__") and c.__tablename__ == data_source_name:
                return c
        raise KeyError(f"{data_source_name} not found in available model classes")

    def write_data_upload_to_backend(self, uploaded_data_df, data_source_name):
        """
        :param uploaded_data_df: pandas dataframe on which we have already done validation
        :param data_source_name:
        Assumption: data_source for this upload is only one table, even though they can generally refer to more than one table

        :return:
        """
        sqlalchemy_model_class = self.get_sqlalchemy_model_class_for_data_source_name(
            data_source_name
        )
        table = sqlalchemy_model_class.__table__
        uploaded_data_df[UPLOAD_ID] = uuid.uuid1()
        # todo: write upload time and other upload information from the form to an uploads metadata table
        upload_time = datetime.utcnow()

        # if we are adding an index pk column, get it from the pandas df
        if INDEX_COLUMN in [c.name for c in table.primary_key]:
            if INDEX_COLUMN not in uploaded_data_df.columns:
                uploaded_data_df = uploaded_data_df.reset_index()
            else:
                # verify that all indices in the existing index column are unique
                num_uploaded_rows = uploaded_data_df.shape[0]
                num_unique_indexes = len(uploaded_data_df[INDEX_COLUMN].unique())
                assert (
                    num_unique_indexes == num_uploaded_rows
                ), f"{INDEX_COLUMN} has non-unique values"
        row_dicts_to_write = uploaded_data_df.to_dict("records")
        # todo: writing all of this to memory- could get gross for large uploads
        row_to_write = [sqlalchemy_model_class(**row) for row in row_dicts_to_write]
        db_session.bulk_save_objects(row_to_write)
        db_session.commit()
        return upload_time

    def write_new_data_file_type(self):
        """
        Handle the case where the user wants to upload a new data file type
        :return:
        """
        raise NotImplementedError


class SqlHandler(DataHandler):
    def __init__(self, data_sources):
        self.data_sources = data_sources
        for data_source in self.data_sources:
            table_name = data_source[DATA_SOURCE_TYPE]
            table_class = self.get_class_name_from_table_name(table_name)
            data_source.update({DATA_LOCATION: table_class})
        self.combined_data_table = self.build_combined_data_table()

    @staticmethod
    def get_class_name_from_table_name(table_name) -> str:
        """
        Camel-cased class name generated by sqlalchemy codegen doesn't match the sql table name. Fetch the class name based on matching class table name
        :param tablename str
        :return: class name
        """
        return Base.metadata.tables[table_name]

    def build_combined_data_table(self):
        """
        This takes the tables specified in self.data_sources and combines them into one easily referenceable selectable.
        This is essentially a query view- performing the joins and getting all columns that can be filtered in a query.

        :return: SqlAlchemy DeclarativeMeta selectable class
        """
        # first build a query that will get all of the columns for all of the requested tables
        query = db_session.query(
            *[data_source[DATA_LOCATION] for data_source in self.data_sources]
        )
        #
        # columns_to_get = {}
        # for data_source in self.data_sources:
        #     table_columns = data_source[DATA_LOCATION].__table__.columns
        #     for column in table_columns:
        #         # we only include the column in the columns to get if it doesn't match the name of an already included column
        #         if column.name not in columns_to_get:
        #             columns_to_get[column.name] = column
        # query = db_session.query(*columns_to_get).select_from(self.data_sources[0][DATA_LOCATION])

        # Assumption: All joins link to IDs in first table, not IDs that were added in previous joins
        for i, data_source in enumerate(self.data_sources):
            table_class = data_source[DATA_LOCATION]
            if i > 0:
                # how do we join in this data source to the previous ones in the query
                previous_data_source = self.data_sources[i - 1]
                previous_table_class = previous_data_source[DATA_LOCATION]
                # todo: join on multiple keys
                matched_keys = zip(data_source[LEFT_KEYS], data_source[RIGHT_KEYS])
                join_clauses = [
                    (
                        getattr(previous_table_class, matched_key[0])
                        == getattr(table_class, matched_key[1])
                    )
                    for matched_key in matched_keys
                ]
                query = query.join(
                    data_source[DATA_LOCATION],
                    and_(
                        *join_clauses
                    ),  # on clause matches corresponding table columns
                )

        class QueryView(Base):
            # defines the selectable class for the query view. Prefixes all column names with the table_name
            # todo: can we avoid this prefixing?
            __table__ = query.selectable.alias()

        return QueryView

    def get_column_names(self):
        """
        :return: a list of the column names in the table referenced by the handler
        """
        # todo: these are prefixed with table_name of the sub table in combined_data_table
        return list(self.combined_data_table().__table__.columns.keys())
        # raise NotImplementedError("This function is not used meaningfully, delete?")
        # return list(self.combined_data_table.columns.keys())

    def get_column_objects_from_config_string(self, columns):

        """
        rename is switching the '_' separation back to '.'
        Look up the right classes
        :return:
        """
        column_rename_dict = {
            config_col.replace(".", "_"): config_col for config_col in columns
        }
        column_mapping_dict = {
            config_col: getattr(self.combined_data_table, database_col)
            for database_col, config_col in column_rename_dict.items()
        }
        return column_rename_dict, column_mapping_dict

    def get_column_data(self, columns: list, filters: dict = None) -> dict:
        """
        :param columns: A complete list of the columns to be returned
        :param filters: Optional dict specifying how to filter the requested columns based on the row values
        :return: a dict keyed by column name and valued with lists of row datapoints for the column
        """
        (
            column_rename_dict,
            column_mapping_dict,
        ) = self.get_column_objects_from_config_string(columns)
        query = db_session.query(*column_mapping_dict.values())

        # todo: implement filters with the refactored version from Alexander
        response_rows = query.all()
        # use pandas to read the sql response and convert to a dict of lists keyed by column names
        # rename is switching the '_' separation back to '.'
        response_as_df = pd.DataFrame(response_rows).rename(columns=column_rename_dict)
        response_dict_of_lists = response_as_df.to_dict(orient="list")
        return response_dict_of_lists

    def get_column_unique_entries(self, cols: list) -> dict:
        """
        :param cols: a list of column names
        :return: A dict keyed by column names and valued with the unique values in that column
        """
        unique_dict = {}
        (
            column_rename_dict,
            column_mapping_dict,
        ) = self.get_column_objects_from_config_string(cols)
        for config_col, sql_col_class in column_mapping_dict.items():
            query = db_session.query(sql_col_class).distinct()
            response = query.all()
            # todo: note we're dropping none/missing values from the response. Do we want to be able to include them?
            unique_dict[config_col] = [r[0] for r in response if r[0] is not None]
        return unique_dict
